{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "Reference : https://github.com/ashishpatel26/LLM-Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Pre-training - General - Trained over large corpora\n",
        "\n",
        "\n",
        "Fine-tuning - Task Specific - trained on smaller task specific data\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "6Xg9dajMfRyh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwcwXRajHelL",
        "outputId": "4d4a5999-9f99-4d11-b00b-2242a3822b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 29 14:11:37 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate transformers einops datasets peft bitsandbytes trl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH8jsePrSGTH",
        "outputId": "40064e77-ac3f-451d-ec51-e584a32dc791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.7.11)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.7.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.15)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.0)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpf1Z0k4RJM6"
      },
      "outputs": [],
      "source": [
        "# !pip install -Uqqq pip --progress-bar off\n",
        "# !pip install -qqq torch==2.0.1 --progress-bar off\n",
        "# !pip install -qqq transformers==4.32.1 --progress-bar off\n",
        "# !pip install -qqq datasets==2.14.4 --progress-bar off\n",
        "# !pip install -qqq peft==0.5.0 --progress-bar off\n",
        "# !pip install -qqq bitsandbytes==0.41.1 --progress-bar off\n",
        "# !pip install -qqq trl==0.7.1 --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1PG0WSvRqVq"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "from pprint import pprint\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from peft import LoraConfig, PeftModel, get_peft_model\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_NAME = \"microsoft/phi-1_5\"\n",
        "# MODEL_NAME = \"openai-community/gpt2-medium\"0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ixsX2Y4doEf"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"Salesforce/dialogstudio\", \"TweetSumm\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kc0CVTtUkWvl",
        "outputId": "6a081c94-40ea-4b15-f7c1-d83c14a79d84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1454: FutureWarning: The repository for Salesforce/dialogstudio contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Salesforce/dialogstudio\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['original dialog id', 'new dialog id', 'dialog index', 'original dialog info', 'log', 'prompt'],\n",
              "        num_rows: 879\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['original dialog id', 'new dialog id', 'dialog index', 'original dialog info', 'log', 'prompt'],\n",
              "        num_rows: 110\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['original dialog id', 'new dialog id', 'dialog index', 'original dialog info', 'log', 'prompt'],\n",
              "        num_rows: 110\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\n",
        "Below is a conversation between a human and an AI agent. Write a summary of the conversation.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def generate_training_prompt(\n",
        "    conversation: str, summary: str, system_prompt: str = DEFAULT_SYSTEM_PROMPT\n",
        ") -> str:\n",
        "    return f\"\"\"### Instruction: {system_prompt}\n",
        "\n",
        "### Input:\n",
        "{conversation.strip()}\n",
        "\n",
        "### Response:\n",
        "{summary}\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "fbx71jhaMGK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    text = re.sub(r\"@[^\\s]+\", \"\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return re.sub(r\"\\^[^ ]+\", \"\", text)\n",
        "\n",
        "\n",
        "def create_conversation_text(data_point):\n",
        "    text = \"\"\n",
        "    for item in data_point[\"log\"]:\n",
        "        user = clean_text(item[\"user utterance\"])\n",
        "        text += f\"user: {user.strip()}\\n\"\n",
        "\n",
        "        agent = clean_text(item[\"system response\"])\n",
        "        text += f\"agent: {agent.strip()}\\n\"\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "gMfzUJVOR9Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(data_point):\n",
        "    summaries = json.loads(data_point[\"original dialog info\"])[\"summaries\"][\n",
        "        \"abstractive_summaries\"\n",
        "    ]\n",
        "    summary = summaries[0]\n",
        "    summary = \" \".join(summary)\n",
        "\n",
        "    conversation_text = create_conversation_text(data_point)\n",
        "    return {\n",
        "        \"conversation\": conversation_text,\n",
        "        \"summary\": summary,\n",
        "        \"text\": generate_training_prompt(conversation_text, summary),\n",
        "    }"
      ],
      "metadata": {
        "id": "eRbskn48QNfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = generate_text(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "J9VuGHGYSR5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(example[\"summary\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycfL7LvHSu2h",
        "outputId": "1e4fed3f-28cc-4f14-e6b4-a5eb1f195a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer enquired about his Iphone and Apple watch which is not showing his any steps/activity and health activities. Agent is asking to move to DM and look into it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(example[\"conversation\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL7EDMq_SxlJ",
        "outputId": "def2a5c9-8b73-4bb9-fe17-4d2ba2ec60f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user: So neither my iPhone nor my Apple Watch are recording my steps/activity, and Health doesn’t recognise either source anymore for some reason. Any ideas? please read the above.\n",
            "agent: Let’s investigate this together. To start, can you tell us the software versions your iPhone and Apple Watch are running currently?\n",
            "user: My iPhone is on 11.1.2, and my watch is on 4.1.\n",
            "agent: Thank you. Have you tried restarting both devices since this started happening?\n",
            "user: I’ve restarted both, also un-paired then re-paired the watch.\n",
            "agent: Got it. When did you first notice that the two devices were not talking to each other. Do the two devices communicate through other apps such as Messages?\n",
            "user: Yes, everything seems fine, it’s just Health and activity.\n",
            "agent: Let’s move to DM and look into this a bit more. When reaching out in DM, let us know when this first started happening please. For example, did it start after an update or after installing a certain app?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(example[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbiL_o5dZZhg",
        "outputId": "d0cfd464-ad2c-4158-d6e2-854e8e07475c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction: Below is a conversation between a human and an AI agent. Write a summary of the conversation.\n",
            "\n",
            "### Input:\n",
            "user: So neither my iPhone nor my Apple Watch are recording my steps/activity, and Health doesn’t recognise either source anymore for some reason. Any ideas? please read the above.\n",
            "agent: Let’s investigate this together. To start, can you tell us the software versions your iPhone and Apple Watch are running currently?\n",
            "user: My iPhone is on 11.1.2, and my watch is on 4.1.\n",
            "agent: Thank you. Have you tried restarting both devices since this started happening?\n",
            "user: I’ve restarted both, also un-paired then re-paired the watch.\n",
            "agent: Got it. When did you first notice that the two devices were not talking to each other. Do the two devices communicate through other apps such as Messages?\n",
            "user: Yes, everything seems fine, it’s just Health and activity.\n",
            "agent: Let’s move to DM and look into this a bit more. When reaching out in DM, let us know when this first started happening please. For example, did it start after an update or after installing a certain app?\n",
            "\n",
            "### Response:\n",
            "Customer enquired about his Iphone and Apple watch which is not showing his any steps/activity and health activities. Agent is asking to move to DM and look into it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(data: Dataset):\n",
        "    return (\n",
        "        data.shuffle(seed=42)\n",
        "        .map(generate_text)\n",
        "        .remove_columns(\n",
        "            [\n",
        "                \"original dialog id\",\n",
        "                \"new dialog id\",\n",
        "                \"dialog index\",\n",
        "                \"original dialog info\",\n",
        "                \"log\",\n",
        "                \"prompt\",\n",
        "            ]\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "jKidKeUpZkPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"] = process_dataset(dataset[\"train\"])\n",
        "dataset[\"validation\"] = process_dataset(dataset[\"validation\"])\n",
        "dataset[\"test\"] = process_dataset(dataset[\"test\"])"
      ],
      "metadata": {
        "id": "XHy1pVjlaLtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usDeQuT2Wssl"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwxKEVnKWWss"
      },
      "outputs": [],
      "source": [
        "def create_model_and_tokenizer():\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_NAME,\n",
        "        device_map={\"\":0},\n",
        "        trust_remote_code=True,\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    return model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDVlGrUmdzdh",
        "outputId": "449bbac0-a517-4f9f-ec03-aa367987c2f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = create_model_and_tokenizer()\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRubXChbe7pL",
        "outputId": "4ceabf93-0dfa-4f55-c1dc-85c4ed5f3ff1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'quant_method': <QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>,\n",
              " 'load_in_8bit': False,\n",
              " 'load_in_4bit': True,\n",
              " 'llm_int8_threshold': 6.0,\n",
              " 'llm_int8_skip_modules': None,\n",
              " 'llm_int8_enable_fp32_cpu_offload': False,\n",
              " 'llm_int8_has_fp16_weight': False,\n",
              " 'bnb_4bit_quant_type': 'nf4',\n",
              " 'bnb_4bit_use_double_quant': True,\n",
              " 'bnb_4bit_compute_dtype': 'float16'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model.config.quantization_config.to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/lora-2-1024x514.png'>\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/lora-3-300x217@2x.png'>\n",
        "\n",
        "\n",
        "<img src='https://lightningaidev.wpengine.com/wp-content/uploads/2023/04/lora-4-300x226@2x.png'>\n",
        "\n",
        "\n",
        "Reference - https://lightning.ai/pages/community/article/lora-llm/"
      ],
      "metadata": {
        "id": "txMnmbaseLPO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Sj65mDxNeJv"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    #target_modules=[\"Wqkv\", \"out_proj\"],target_modules=[\"fc1\", \"fc2\",\"Wqkv\", \"out_proj\"]\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9_pev-FJQUT",
        "outputId": "093daadb-b4b0-4bf4-a241-15d1acbb513d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PhiForCausalLM(\n",
              "  (model): PhiModel(\n",
              "    (embed_tokens): Embedding(51200, 2048)\n",
              "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x PhiDecoderLayer(\n",
              "        (self_attn): PhiAttention(\n",
              "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "          (dense): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "          (rotary_emb): PhiRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): PhiMLP(\n",
              "          (activation_fn): NewGELUActivation()\n",
              "          (fc1): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv11jEAJT9Pp",
        "outputId": "253c35d1-086b-4142-a333-7416cf4226e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 11,010,048 || all params: 1,429,280,768 || trainable%: 0.7703208667256062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVSLIcQl8g9h"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIoe9TqOof-V"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(\n",
        "        output_dir=\"phi-1_5-finetuned-dialogstudio\",\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=1,\n",
        "        learning_rate=2e-4,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=1,\n",
        "        max_steps=3,\n",
        "        num_train_epochs=1,\n",
        "        push_to_hub=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZL4Qa_cypL1f",
        "outputId": "12fa38a8-1f2f-4ea5-9a78-c817a6111ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:225: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "y7LDwR6kJdkO",
        "outputId": "dcc1e5b1-3b71-4b5d-e354-86a10f1d4569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:01, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.149500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.875400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.811800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory phi-1_5-finetuned-dialogstudio/checkpoint-3 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=2.9455955028533936, metrics={'train_runtime': 4.678, 'train_samples_per_second': 0.641, 'train_steps_per_second': 0.641, 'total_flos': 8463064043520.0, 'train_loss': 2.9455955028533936, 'epoch': 0.0})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "IRiQABpzeom6",
        "outputId": "eb968a32-aa21-40e5-874c-76108faacd84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [14/14 00:11]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 3.2396492958068848,\n",
              " 'eval_runtime': 12.8552,\n",
              " 'eval_samples_per_second': 8.557,\n",
              " 'eval_steps_per_second': 1.089,\n",
              " 'epoch': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"phi-finetuned-dialogstudio\")"
      ],
      "metadata": {
        "id": "jJdkDvxKOq8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WywDQRmGEVOJ",
        "outputId": "67230540-e301-4a9c-bfa9-cc52cf70ba89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): PhiForCausalLM(\n",
              "      (model): PhiModel(\n",
              "        (embed_tokens): Embedding(51200, 2048)\n",
              "        (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (layers): ModuleList(\n",
              "          (0-23): 24 x PhiDecoderLayer(\n",
              "            (self_attn): PhiAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (dense): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
              "              (rotary_emb): PhiRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): PhiMLP(\n",
              "              (activation_fn): NewGELUActivation()\n",
              "              (fc1): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=8192, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (fc2): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=8192, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "            )\n",
              "            (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "trainer.model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "7lSxtsWehGcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM\n",
        "import torch\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True, torch_dtype=torch.float32)\n",
        "peft_model = PeftModel.from_pretrained(model, \"/content/gpt-2-finetuned-dialogstudio\", from_transformers=True)\n",
        "model = peft_model.merge_and_unload()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e43OUEfYOzFF",
        "outputId": "15f04550-e4dd-44a0-d0d2-4ce1aa4cb136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PhiForCausalLM(\n",
              "  (model): PhiModel(\n",
              "    (embed_tokens): Embedding(51200, 2048)\n",
              "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x PhiDecoderLayer(\n",
              "        (self_attn): PhiAttention(\n",
              "          (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          (dense): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "          (rotary_emb): PhiRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): PhiMLP(\n",
              "          (activation_fn): NewGELUActivation()\n",
              "          (fc1): Linear(in_features=2048, out_features=8192, bias=True)\n",
              "          (fc2): Linear(in_features=8192, out_features=2048, bias=True)\n",
              "        )\n",
              "        (input_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (final_layernorm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=51200, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"ashishpatel26/phi-1_5-finetuned-dialogstudio\", trust_remote_code=True, torch_dtype=torch.float32)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
        "inputs = tokenizer(f'''{dataset[\"test\"]['text'][0]}''', return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=512)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ],
      "metadata": {
        "id": "vpbzxK0zhHk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b35f612-bfa7-423e-eee0-d6bbf72e856c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction: Below is a conversation between a human and an AI agent. Write a summary of the conversation.\n",
            "\n",
            "### Input:\n",
            "user: looking to change my flight Friday, Oct 27. GRMSKV to DL4728 from SLC to ORD. Is that an option and what is the cost? Jess\n",
            "agent: The difference in fare is $185.30. This would include all airport taxes and fees. The ticket is non-refundable changeable with a fee, *ALS and may result in additional fare collection for changes when making a future changes. *ALS\n",
            "user: I had a first class seat purchased for the original flight, would that be the same with this flight to Chicago?\n",
            "agent: Hello, Jess. That is the fare difference. You will have to call us at 1 800 221 1212 to make any changes. It is in First class. *TAY\n",
            "user: thx\n",
            "agent: Our pleasure. *ALS\n",
            "user: Do I have to call or is there a means to do this online?\n",
            "agent: You can call or you can login to your trip on our website to make changes. *TJE\n",
            "\n",
            "### Response:\n",
            "Customer is looking to change the flight on Friday Oct 27 is that an option and asking about cost. Agent replying that there is an difference in fare and this would include all airport taxes and fees and ticket is non refundable changeable with a fee. The ticket is non-refundable changeable with a fee, and the ticket is non-refundable changeable with a fee, and may result in additional fare collection for changes when making a future changes. The ticket is non-refundable changeable with a fee, and the ticket is non-refundable changeable with a fee, and may result in additional fare collection for changes when making a future changes. The ticket is non-refundable changeable with a fee, and the ticket is non-refundable changeable with a fee, and may result in additional fare collection for changes when making a future changes. The ticket is non-refundable changeable with a fee, and the ticket is non-refundable changeable with a fee, and may result in additional fare collection for changes when making a future changes. The ticket is non-refundable changeable with a fee, and the ticket is non-refundable changeable with a fee, and may result in additional fare collection for changes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset[\"test\"]['text'][0]"
      ],
      "metadata": {
        "id": "oU9augJbhm44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfxoP-IvnheQ"
      },
      "source": [
        "## References\n",
        "\n",
        "- https://huggingface.co/datasets/Salesforce/dialogstudio\n",
        "- https://huggingface.co/meta-llama/Llama-2-7b-hf"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "vVSLIcQl8g9h",
        "7lSxtsWehGcy"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}