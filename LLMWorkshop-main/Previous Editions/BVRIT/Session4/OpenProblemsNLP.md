As you scale LLMs, they can become very effective. So what is left in NLP research?

- Efficient Methods
	- Data annotation
	- Downstream Task Adaptation - LoRA
	- Model architecture and input


- Small scale problems
	- Moving away from blockbox nature towards interpretability
	- DPO tested on GPT-2 + IMDB reviews dataset
	- Proposing new methods with small scale validation 

- Data Constrained Settings
	- Not every language has web-scale data. 
	- Even for high resource language we are reaching limits of webscale.
	- Multilinguality and Low-resource 

- Evaluation
	- Are we climbing the correct ladder?
	- Are today's benchmarks really indicative of intelligence?
	- Do we have automatic metrics to match human judgements?
	
- Reasoning
	- Do LLMs have strong formal logic, and are consistent with their reasoning?
	- Responsible reasoning in social contexts
	- Analyzing how prompts help reasoning
	
- Knowledge Bases
	- Knowledge-guided LLM
	- Automatic knowledge base construction
	- General and Cultural Commonsense
	
	
References
- [Seb Ruder's Blog - NLP Research in the Era of LLMs](https://newsletter.ruder.io/p/nlp-research-in-the-era-of-llms)
- [A PhD Studentâ€™s Perspective on Research in NLP in the Era of Very Large Language Models](https://arxiv.org/pdf/2305.12544.pdf)
- [Choose Your Weapon: Survival Strategies for Depressed AI Academics](https://arxiv.org/pdf/2304.06035.pdf)





